{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbASEb83dTBOJQzcpwKHr2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranavsrinivas29/Language-Models/blob/main/Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXfOeKcXCxjx",
        "outputId": "bfff36ac-c2d8-4700-98a3-a05b059fd2d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Statistical Language Model (n-gram)**"
      ],
      "metadata": {
        "id": "ZpouZtnZIyLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import bigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Example corpus\n",
        "corpus = \"We are about to demonstrate a simple language model. We will show how statistical models can predict the next word.\"\n",
        "\n",
        "# Tokenization and bigram model preparation\n",
        "nltk.download('punkt')\n",
        "tokens = nltk.word_tokenize(corpus.lower())\n",
        "bigram_counts = Counter(bigrams(tokens))\n",
        "total_counts = len(tokens)\n",
        "\n",
        "# Predict next word\n",
        "def predict_next_word_statistical(previous_word):\n",
        "    possibilities = {pair[1]: count for pair, count in bigram_counts.items() if pair[0] == previous_word}\n",
        "    if possibilities:\n",
        "        return max(possibilities, key=possibilities.get)\n",
        "    else:\n",
        "        return \"No prediction available\"\n",
        "\n",
        "# Example usage\n",
        "print(\"Statistical Model Prediction:\", predict_next_word_statistical(\"language\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgfUUTYGDqSN",
        "outputId": "dee2d1ed-896d-4e0f-da7a-8f9124d5fe73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Model Prediction: model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Statistical Model Prediction:\", predict_next_word_statistical(\"we\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD7HZpzVDvTk",
        "outputId": "b2ecfcce-2620-4761-eb7d-584aee9c3f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical Model Prediction: are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Example corpus and tokenization\n",
        "corpus = \"We are about to demonstrate a simple language model. We will show how statistical models can predict the next word.\"\n",
        "tokens = nltk.word_tokenize(corpus.lower())\n",
        "fourgram_counts = Counter(ngrams(tokens, 4))\n",
        "threegram_counts = Counter(ngrams(tokens, 3))\n",
        "\n",
        "# Function to predict next word based on previous three words\n",
        "def predict_next_word_statistical_4gram(word1, word2, word3):\n",
        "    possibilities = {(pair[1], pair[2], pair[3]): count for pair, count in fourgram_counts.items() if pair[0] == word1 and pair[1] == word2 and pair[2] == word3}\n",
        "    if possibilities:\n",
        "        return max(possibilities, key=possibilities.get)[2]  # Return the fourth word of the most common 4-gram\n",
        "    else:\n",
        "        return \"No prediction available\"\n",
        "\n",
        "# Example usage\n",
        "print(\"Statistical 4-gram Model Prediction:\", predict_next_word_statistical_4gram(\"a\", \"simple\", \"language\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heN8Or1nEUlu",
        "outputId": "155f81e1-470d-42bd-fa3d-f221f3e568f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Statistical 4-gram Model Prediction: model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Language Model**"
      ],
      "metadata": {
        "id": "zo0_-2_YI9R1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([corpus])\n",
        "sequences = tokenizer.texts_to_sequences([corpus])[0]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Create sequences\n",
        "input_sequences = []\n",
        "for i in range(1, len(sequences)):\n",
        "    n_gram_sequence = sequences[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Predictors and label\n",
        "X, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "y = tf.keras.utils.to_categorical(labels, num_classes=vocab_size)\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, 10, input_length=max_sequence_len-1),\n",
        "    LSTM(150),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=0)\n",
        "\n",
        "def predict_next_word_neural(previous_text):\n",
        "    token_list = tokenizer.texts_to_sequences([previous_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predictions = model.predict(token_list, verbose=0)\n",
        "    predicted_index = np.argmax(predictions, axis=-1)[0]\n",
        "    predicted_word = tokenizer.index_word[predicted_index] if predicted_index in tokenizer.index_word else \"No prediction available\"\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage, ensuring \"we are about to\" is part of the training corpus for meaningful prediction\n",
        "print(\"Neural Model Prediction:\", predict_next_word_neural(\"we are not going\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c17XUkXAEkNG",
        "outputId": "c157f130-4541-4f99-f065-758fb378b133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Model Prediction: simple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  **LLM**"
      ],
      "metadata": {
        "id": "T_ioBjmNG-_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Ensure the tokenizer's pad token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def left_pad_sequence(sequence, max_length, pad_token):\n",
        "    \"\"\"\n",
        "    Manually pads the sequence from the left to a specified max_length with the given pad_token.\n",
        "    \"\"\"\n",
        "    padding_length = max_length - len(sequence)\n",
        "    if padding_length > 0:\n",
        "        return [pad_token] * padding_length + sequence\n",
        "    else:\n",
        "        return sequence\n",
        "\n",
        "def generate_text_simple(prompt_text, max_length=100):\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(prompt_text, add_special_tokens=True)\n",
        "\n",
        "    # Manually apply left-side padding\n",
        "    padded_input_ids = left_pad_sequence(input_ids, max_length, tokenizer.pad_token_id)\n",
        "\n",
        "    # Convert to tensors\n",
        "    input_ids_tensor = torch.tensor([padded_input_ids])\n",
        "\n",
        "    # Generate a sequence of text with attention mask\n",
        "    attention_mask = [0 if token == tokenizer.pad_token_id else 1 for token in padded_input_ids]\n",
        "    attention_mask_tensor = torch.tensor([attention_mask])\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(input_ids_tensor, attention_mask=attention_mask_tensor, max_length=max_length, pad_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "generated_text = generate_text_simple(\"We are about to\", max_length=100)\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-jFOqzlG-fP",
        "outputId": "49fcdd48-4cd4-4580-c82c-c35d27822485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: We are about to launch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT**"
      ],
      "metadata": {
        "id": "7msv3Ce2IqRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Function to predict next words\n",
        "def predict_next_word_large_model(prompt_text):\n",
        "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt')\n",
        "    beam_output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    return tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example usage\n",
        "print(\"Large Model (GPT) Prediction:\", predict_next_word_large_model(\"We are not going\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiVy_EUqE8-S",
        "outputId": "d4a4d4af-fab2-4b2b-da0f-9b5600ec251b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Large Model (GPT) Prediction: We are not going to be able to do that. We are going through a period of time where we are trying to figure out how to get the best out of our players.\n",
            "\n",
            "\"We have to make sure we have the right players in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT2LMHeadModel**: This is the GPT-2 model class that includes the language modeling head on top of the base GPT-2 architecture. The language modeling head allows the model to generate text.\n",
        "\n",
        "**GPT2Tokenizer**: This class is responsible for converting text into a format that the GPT-2 model can understand (i.e., tokenizing the text into numbers).\n",
        "\n",
        "**from_pretrained**('gpt2'): This method loads the pre-trained GPT-2 model and tokenizer. The 'gpt2' identifier refers to the default version of the GPT-2 model, which is trained on a wide variety of internet text.\n",
        "\n",
        "**tokenizer.encode()**: Converts the prompt_text into a list of token IDs using the GPT-2 tokenizer. These token IDs represent the numerical values assigned to each piece of the text.\n",
        "\n",
        "**return_tensors='pt'**: Specifies that the output should be a PyTorch tensor.\n",
        "python\n",
        "\n",
        "\n",
        "**model.generate()**: Generates text based on the provided input_ids. It uses several parameters to control the generation process:\n",
        "**max_length=50**: The maximum length of the sequence to generate, including the given prompt text.\n",
        "\n",
        "**num_return_sequences=1**: The number of generated sequences to return. Here, it's set to 1, meaning only one sequence will be generated.\n",
        "\n",
        "**no_repeat_ngram_size=2**: Ensures that no 2-gram (sequence of two tokens) repeats within the generated text, helping to increase the diversity of the generated text.\n",
        "\n",
        "**early_stopping**=True: Stops generation when all sequences reach the end of the sequence token (<EOS>).\n",
        "\n",
        "\n",
        "**tokenizer.decode()**: Converts the generated token IDs back into a string of text. The skip_special_tokens=True argument tells the decoder to ignore special tokens, such as padding tokens or other non-text tokens, making the output cleaner and more readable."
      ],
      "metadata": {
        "id": "DZZWWZlmGCZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perplexity**"
      ],
      "metadata": {
        "id": "L63-qDiOODMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "# Ensure nltk's 'punkt' tokenizer models are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = \"language\"\n",
        "\n",
        "# Calculate N-gram and (N-1)-gram counts for 2-gram and 4-gram models\n",
        "fourgram_counts = Counter(ngrams(tokens, 4))\n",
        "threegram_counts = Counter(ngrams(tokens, 3))\n",
        "twogram_counts = Counter(ngrams(tokens, 2))\n",
        "unigram_counts = Counter(ngrams(tokens, 1))\n",
        "\n",
        "def calculate_ngram_perplexity(ngram_counts, previous_ngram_counts):\n",
        "    log_prob = 0\n",
        "    N = 0\n",
        "    for ngram in ngram_counts:\n",
        "        if len(ngram) > 1:\n",
        "            previous_ngram = ngram[:-1]\n",
        "            prob = ngram_counts[ngram] / previous_ngram_counts[previous_ngram] if previous_ngram_counts[previous_ngram] > 0 else 1e-5\n",
        "            log_prob += math.log(prob, 2) * ngram_counts[ngram]\n",
        "            N += ngram_counts[ngram]\n",
        "    perplexity = math.pow(2, -log_prob / N)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate and print perplexities\n",
        "fourgram_perplexity = calculate_ngram_perplexity(fourgram_counts, threegram_counts)\n",
        "twogram_perplexity = calculate_ngram_perplexity(twogram_counts, unigram_counts)\n",
        "\n",
        "print(f\"2-gram Model Perplexity: {twogram_perplexity}\")\n",
        "print(f\"4-gram Model Perplexity: {fourgram_perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mee55qQKx6Y",
        "outputId": "6b27a5db-5897-4eba-de87-f9c62685c5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2-gram Model Perplexity: 1.4859942891369484\n",
            "4-gram Model Perplexity: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}